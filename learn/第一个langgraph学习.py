#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
LangGraph ç®€å•å­¦ä¹ æ¡ˆä¾‹
ä½¿ç”¨ Ollama ä½œä¸º LLM æä¾›è€…
æœåŠ¡å™¨: 192.168.3.3:11434
æ¨¡å‹: qwq:latest

==================================
WHY - ä¸ºä»€ä¹ˆä½¿ç”¨LangGraph:
==================================
LangGraphæ˜¯ä¸€ä¸ªä¸“ä¸ºæ„å»ºåŸºäºLLMçš„å¤šä»£ç†åº”ç”¨è®¾è®¡çš„æ¡†æ¶ï¼Œå®ƒæä¾›äº†ä»¥ä¸‹ä¼˜åŠ¿ï¼š
1. çŠ¶æ€ç®¡ç†ï¼šæœ‰æ•ˆç®¡ç†å¯¹è¯å’Œå¤„ç†è¿‡ç¨‹ä¸­çš„çŠ¶æ€
2. æµç¨‹æ§åˆ¶ï¼šä½¿ç”¨å›¾ç»“æ„å®šä¹‰æ¸…æ™°çš„å¤„ç†æµç¨‹
3. å¯ç»„åˆæ€§ï¼šè½»æ¾ç»„åˆä¸åŒçš„å¤„ç†èŠ‚ç‚¹å½¢æˆå¤æ‚å·¥ä½œæµ
4. å¯æ‰©å±•æ€§ï¼šæ–¹ä¾¿æ‰©å±•æ·»åŠ æ–°çš„åŠŸèƒ½èŠ‚ç‚¹
5. æ˜“äºè°ƒè¯•ï¼šæ¸…æ™°çš„æµç¨‹å¯è§†åŒ–å’ŒçŠ¶æ€è·Ÿè¸ª

==================================
WHAT - æœ¬ä»£ç å®ç°äº†ä»€ä¹ˆ:
==================================
æœ¬ç¤ºä¾‹å®ç°äº†ä¸€ä¸ªç®€å•çš„èŠå¤©æœºå™¨äººï¼Œå®ƒåŒ…å«ä»¥ä¸‹åŠŸèƒ½ï¼š
1. ä½¿ç”¨Ollamaä½œä¸ºå¤§è¯­è¨€æ¨¡å‹æä¾›è€…
2. å®šä¹‰äº†ä¸€ä¸ªåŒèŠ‚ç‚¹å·¥ä½œæµï¼šäººç±»è¾“å…¥ -> AIåŠ©æ‰‹å›å¤
3. æ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„çŠ¶æ€ç®¡ç†æœºåˆ¶
4. å®ç°äº†åŸºæœ¬çš„å¯¹è¯äº¤äº’å¾ªç¯
5. å¼‚å¸¸å¤„ç†ä¸é€€å‡ºæœºåˆ¶

==================================
HOW - å®ç°æ–¹å¼ä¸æ¶æ„:
==================================
æœ¬ä¾‹ä½¿ç”¨LangGraphæ„å»ºæœ‰å‘å›¾å·¥ä½œæµï¼š
1. çŠ¶æ€è®¾è®¡ï¼šä½¿ç”¨TypedDictå®šä¹‰èŠå¤©çŠ¶æ€ï¼ŒåŒ…å«æ¶ˆæ¯å†å²
2. èŠ‚ç‚¹å®šä¹‰ï¼šåˆ†åˆ«å®šä¹‰humanå’Œassistantä¸¤ä¸ªæ ¸å¿ƒèŠ‚ç‚¹
3. å›¾æ„å»ºï¼šè®¾ç½®èŠ‚ç‚¹å’ŒèŠ‚ç‚¹é—´çš„è¿æ¥å…³ç³»
4. è¿è¡Œæ—¶ï¼šé€šè¿‡å¾ªç¯ä¸æ–­è°ƒç”¨å›¾è¿›è¡Œå¯¹è¯äº¤äº’

æ•´ä½“æ¶æ„éµå¾ªäº†"çŠ¶æ€-èŠ‚ç‚¹-å›¾-æ‰§è¡Œ"çš„LangGraphæ ‡å‡†æ¨¡å¼ï¼Œ
é€‚åˆå­¦ä¹ è€…ç†è§£LangGraphçš„åŸºæœ¬æ¦‚å¿µå’Œä½¿ç”¨æ–¹æ³•ã€‚
"""

import os
import sys
import requests
from typing import Dict, TypedDict, Annotated, List, Tuple

from langchain_core.messages import AIMessage, HumanMessage
from langgraph.graph import END, StateGraph
from langchain_core.runnables import RunnableConfig
# ä½¿ç”¨langchain_ollamaæ›¿ä»£å·²å¼ƒç”¨çš„Ollamaç±»
from langchain_ollama import OllamaLLM

# =====================================================================
# çŠ¶æ€å®šä¹‰
# =====================================================================
# ChatStateå®šä¹‰äº†å›¾æ‰§è¡Œè¿‡ç¨‹ä¸­çš„çŠ¶æ€æ•°æ®ç»“æ„
# messages: å­˜å‚¨å¯¹è¯å†å²è®°å½•ï¼ŒåŒ…å«äººç±»æ¶ˆæ¯å’ŒAIå›å¤
class ChatState(TypedDict):
    messages: List[HumanMessage | AIMessage]  # èŠå¤©å†å²è®°å½•

# =====================================================================
# æœåŠ¡å™¨è¿æ¥æµ‹è¯•
# =====================================================================
def test_ollama_connection(base_url, timeout=5):
    """æµ‹è¯•ä¸OllamaæœåŠ¡å™¨çš„è¿æ¥
    
    å‚æ•°:
        base_url: OllamaæœåŠ¡å™¨åœ°å€
        timeout: è¿æ¥è¶…æ—¶æ—¶é—´(ç§’)
        
    è¿”å›:
        æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
    """
    try:
        print(f"æ­£åœ¨è¿æ¥OllamaæœåŠ¡å™¨: {base_url}")
        response = requests.get(f"{base_url}/api/tags", timeout=timeout)
        if response.status_code == 200:
            print("âœ… æˆåŠŸè¿æ¥åˆ°OllamaæœåŠ¡å™¨!")
            available_models = [model["name"] for model in response.json()["models"]]
            print(f"å¯ç”¨æ¨¡å‹: {', '.join(available_models)}")
            return True
        else:
            print(f"âŒ æœåŠ¡å™¨è¿”å›é”™è¯¯: {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡å™¨ï¼Œè¯·ç¡®è®¤æœåŠ¡å™¨åœ°å€æ˜¯å¦æ­£ç¡®")
        return False
    except requests.exceptions.Timeout:
        print("âŒ è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–å¢åŠ è¶…æ—¶æ—¶é—´")
        return False
    except Exception as e:
        print(f"âŒ è¿æ¥å¼‚å¸¸: {str(e)}")
        return False

# =====================================================================
# LLMé…ç½®
# =====================================================================
# OllamaæœåŠ¡å™¨åœ°å€
OLLAMA_BASE_URL = "http://192.168.3.3:11434"
MODEL_NAME = "qwq:latest"

# åˆ›å»ºOllama LLMå®ä¾‹ï¼Œè¿æ¥åˆ°æŒ‡å®šæœåŠ¡å™¨ä¸Šçš„ç‰¹å®šæ¨¡å‹
# base_url: OllamaæœåŠ¡å™¨åœ°å€
# model: ä½¿ç”¨çš„æ¨¡å‹åç§°
# temperature: æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼Œå€¼è¶Šé«˜å›å¤è¶Šå¤šæ ·åŒ–
llm = OllamaLLM(
    base_url=OLLAMA_BASE_URL,       # OllamaæœåŠ¡å™¨åœ°å€
    model=MODEL_NAME,               # ä½¿ç”¨çš„æ¨¡å‹
    temperature=0.7,                # æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§
    request_timeout=20.0,           # è¯·æ±‚è¶…æ—¶è®¾ç½®(ç§’)
)

# =====================================================================
# èŠ‚ç‚¹å‡½æ•°å®šä¹‰
# =====================================================================

# åŠ©æ‰‹èŠ‚ç‚¹ï¼šè´Ÿè´£å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶ç”ŸæˆAIå›å¤
def assistant(state: ChatState, config: RunnableConfig) -> ChatState:
    """LLM åŠ©æ‰‹å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶ç”Ÿæˆå›å¤
    
    å‚æ•°:
        state: å½“å‰çŠ¶æ€ï¼ŒåŒ…å«æ¶ˆæ¯å†å²
        config: è¿è¡Œæ—¶é…ç½®å‚æ•°
        
    è¿”å›:
        æ›´æ–°åçš„çŠ¶æ€ï¼ŒåŒ…å«æ–°å¢çš„AIå›å¤
    """
    messages = state["messages"]
    
    print("ğŸ¤– AIåŠ©æ‰‹æ­£åœ¨æ€è€ƒ...")
    try:
        # ä½¿ç”¨LLMæ¨¡å‹å¤„ç†æ¶ˆæ¯å†å²å¹¶ç”Ÿæˆå›å¤
        # invokeæ–¹æ³•å°†æ•´ä¸ªæ¶ˆæ¯å†å²ä¼ é€’ç»™æ¨¡å‹ä»¥ä¿æŒä¸Šä¸‹æ–‡
        response = llm.invoke(messages)
        print(f"ğŸ¤– AIå›å¤: {response}")
        
        # å°†ç”Ÿæˆçš„å›å¤æ·»åŠ åˆ°æ¶ˆæ¯å†å²ä¸­ï¼Œä½¿ç”¨AIMessageåŒ…è£…
        messages.append(AIMessage(content=response))
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}")
        # æ·»åŠ ä¸€ä¸ªé”™è¯¯æ¶ˆæ¯ï¼Œç¡®ä¿ç¨‹åºå¯ä»¥ç»§ç»­è¿è¡Œ
        messages.append(AIMessage(content=f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€ä¸ªé”™è¯¯: {str(e)}"))
    
    # è¿”å›æ›´æ–°åçš„çŠ¶æ€
    return {"messages": messages}

# äººç±»èŠ‚ç‚¹ï¼šè´Ÿè´£è·å–ç”¨æˆ·è¾“å…¥
def human(state: ChatState) -> ChatState:
    """å¤„ç†äººç±»è¾“å…¥ï¼Œè·å–ç”¨æˆ·æ¶ˆæ¯å¹¶æ·»åŠ åˆ°çŠ¶æ€ä¸­
    
    å‚æ•°:
        state: å½“å‰çŠ¶æ€ï¼ŒåŒ…å«æ¶ˆæ¯å†å²
        
    è¿”å›:
        æ›´æ–°åçš„çŠ¶æ€ï¼ŒåŒ…å«æ–°å¢çš„ç”¨æˆ·æ¶ˆæ¯
    """
    messages = state["messages"]
    
    # é€šè¿‡æ§åˆ¶å°è·å–ç”¨æˆ·è¾“å…¥
    user_input = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ")
    
    # å°†ç”¨æˆ·è¾“å…¥å°è£…ä¸ºHumanMessageå¹¶æ·»åŠ åˆ°æ¶ˆæ¯å†å²
    messages.append(HumanMessage(content=user_input))
    
    # è¿”å›æ›´æ–°åçš„çŠ¶æ€
    return {"messages": messages}

# =====================================================================
# å›¾æ„å»ºå‡½æ•°
# =====================================================================

def build_graph():
    """åˆ›å»ºå¹¶é…ç½®LangGraphå·¥ä½œæµå›¾
    
    æ„å»ºå›¾çš„æ­¥éª¤:
    1. åˆå§‹åŒ–çŠ¶æ€å›¾
    2. æ·»åŠ å¤„ç†èŠ‚ç‚¹
    3. å®šä¹‰èŠ‚ç‚¹é—´è¿æ¥å…³ç³»
    4. æŒ‡å®šå…¥å£ç‚¹
    5. ç¼–è¯‘å›¾
    
    è¿”å›:
        ç¼–è¯‘åçš„å¯æ‰§è¡Œå›¾
    """
    # åˆå§‹åŒ–çŠ¶æ€å›¾ï¼ŒæŒ‡å®šçŠ¶æ€ç±»å‹ä¸ºChatState
    workflow = StateGraph(ChatState)
    
    # æ·»åŠ å¤„ç†èŠ‚ç‚¹ï¼Œå°†å‡½æ•°ä¸èŠ‚ç‚¹åç§°å…³è”
    workflow.add_node("human", human)         # äººç±»è¾“å…¥èŠ‚ç‚¹
    workflow.add_node("assistant", assistant) # AIåŠ©æ‰‹èŠ‚ç‚¹
    
    # æ·»åŠ è¾¹ï¼Œå®šä¹‰humanèŠ‚ç‚¹ä¸assistantèŠ‚ç‚¹çš„è¿æ¥
    # è¡¨ç¤ºç”¨æˆ·è¾“å…¥åï¼Œä¸‹ä¸€æ­¥æ‰§è¡ŒassistantèŠ‚ç‚¹
    workflow.add_edge("human", "assistant")
    
    # æ·»åŠ ä»assistantå›åˆ°humançš„è¾¹ï¼Œå½¢æˆå¯¹è¯å¾ªç¯
    # è¡¨ç¤ºAIå›å¤åï¼Œç»§ç»­è·å–ç”¨æˆ·è¾“å…¥
    workflow.add_edge("assistant", "human")
    
    # è®¾ç½®å›¾çš„å…¥å£ç‚¹ä¸ºhumanèŠ‚ç‚¹
    # è¡¨ç¤ºæ¯è½®å¯¹è¯éƒ½ä»è·å–ç”¨æˆ·è¾“å…¥å¼€å§‹
    workflow.set_entry_point("human")
    
    # ç¼–è¯‘å·¥ä½œæµå›¾ï¼Œè½¬æ¢ä¸ºå¯æ‰§è¡Œçš„è®¡ç®—å›¾
    return workflow.compile()

# =====================================================================
# ä¸»ç¨‹åº
# =====================================================================

def main():
    """ä¸»å‡½æ•°ï¼Œåˆå§‹åŒ–å¹¶è¿è¡ŒèŠå¤©å¯¹è¯å¾ªç¯
    
    æµç¨‹:
    1. æ„å»ºå›¾
    2. åˆå§‹åŒ–çŠ¶æ€
    3. è¿›å…¥å¯¹è¯å¾ªç¯
    4. å¤„ç†å¼‚å¸¸å’Œé€€å‡ºæ¡ä»¶
    """
    # æ‰“å°æ¬¢è¿ä¿¡æ¯
    print("==== LangGraph èŠå¤©ç¤ºä¾‹ ====")
    print(f"å°è¯•è¿æ¥ Ollama ({OLLAMA_BASE_URL}) çš„ {MODEL_NAME} æ¨¡å‹")
    print("è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º")
    
    # æµ‹è¯•ä¸OllamaæœåŠ¡å™¨çš„è¿æ¥
    if not test_ollama_connection(OLLAMA_BASE_URL):
        print("\nâŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡å™¨ï¼Œæ‚¨å¯ä»¥:")
        print("1. æ£€æŸ¥æœåŠ¡å™¨åœ°å€æ˜¯å¦æ­£ç¡®")
        print("2. ç¡®è®¤OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ")
        print("3. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("4. ä¿®æ”¹ä»£ç ä¸­çš„OLLAMA_BASE_URLä¸ºæ‚¨çš„OllamaæœåŠ¡å™¨åœ°å€")
        
        use_local = input("\næ˜¯å¦å°è¯•ä½¿ç”¨æœ¬åœ°OllamaæœåŠ¡å™¨ (http://localhost:11434)? (y/n): ")
        if use_local.lower() == 'y':
            global llm
            new_base_url = "http://localhost:11434"
            
            if test_ollama_connection(new_base_url):
                # é‡æ–°åˆ›å»ºLLMå®ä¾‹ï¼Œä½¿ç”¨æœ¬åœ°æœåŠ¡å™¨
                llm = OllamaLLM(
                    base_url=new_base_url,
                    model="llama3", # å°è¯•ä½¿ç”¨ä¸€ä¸ªå¸¸è§æ¨¡å‹
                    temperature=0.7,
                    request_timeout=20.0,
                )
            else:
                print("ä»ç„¶æ— æ³•è¿æ¥ï¼Œé€€å‡ºç¨‹åºã€‚")
                return
        else:
            print("é€€å‡ºç¨‹åºã€‚")
            return
    
    # æ„å»ºLangGraphå·¥ä½œæµå›¾
    graph = build_graph()
    
    # åˆå§‹åŒ–çŠ¶æ€ï¼Œç©ºæ¶ˆæ¯åˆ—è¡¨
    state = {"messages": []}
    
    try:
        # æ‰§è¡Œå·¥ä½œæµç›´åˆ°é‡åˆ°é€€å‡ºæ¡ä»¶
        while True:
            # é€šè¿‡è½®è¯¢æ–¹å¼æ£€æŸ¥ç”¨æˆ·æ˜¯å¦è¦é€€å‡º
            # è·å–æœ€è¿‘çš„ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            if state["messages"]:
                last_human_message = next((msg.content for msg in reversed(state["messages"]) 
                                          if isinstance(msg, HumanMessage)), "")
                if last_human_message.lower() in ["exit", "quit", "é€€å‡º"]:
                    print("å†è§!")
                    break
            
            # ä½¿ç”¨invokeæ–¹æ³•è¿è¡Œå›¾ï¼ˆæ›¿ä»£å·²ä¸å­˜åœ¨çš„stepæ–¹æ³•ï¼‰
            # å°†å½“å‰çŠ¶æ€ä¼ å…¥å›¾ä¸­æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„èŠ‚ç‚¹åºåˆ—
            state = graph.invoke(state)
                
    except KeyboardInterrupt:
        # å¤„ç†ç”¨æˆ·é€šè¿‡Ctrl+Cä¸­æ–­
        print("\nç¨‹åºè¢«ä¸­æ–­ï¼Œé€€å‡ºä¸­...")
    except Exception as e:
        # å¤„ç†å…¶ä»–å¼‚å¸¸
        print(f"å‘ç”Ÿé”™è¯¯: {e}")

# ç¨‹åºå…¥å£
if __name__ == "__main__":
    main()
